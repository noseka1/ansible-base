odf_operator_subscription_channel: ''
odf_operator_subscription_approval: Manual
odf_operator_subscription_starting_csv: ''

odf_namespace: openshift-storage

odf_aws:
  # ODF cluster requires at least 4xlarge to fully deploy
  instance_type: m6i.4xlarge
  # ODF requires at least three storage nodes to deploy
  availability_zones: [ a, b, c ]
  nodes_per_zone: 1

# Choose one of the resource profiles:
# 1. Lean mode: Minimizes resource consumption by allocating fewer CPUs and less memory for resource-efficient operations.
# 2. Balanced mode (default): Optimized for right amount of CPU and memory resources to support diverse workloads.
# 3. Performance mode: Tailored for high-performance, allocating ample CPUs and memory to ensure optimal execution of demanding workloads.
#
# Resource profile definitions can be found at:
# https://github.com/red-hat-storage/ocs-operator/blob/d54f77ab27992184d66a5c6901cdc3ca457ae079/controllers/defaults/resources.go
#
# For production clusters, choose balanced or performance
odf_resource_profile: lean

# Choose number of monitors, typically 3 or 5
# Prefer deploying 5 monitors over 3 if possible for increased resiliency. With 5 monitors you can tolerate failure of 2 monitors.
# Each monitor is placed in a different failure domain. 5 monitors require a cluster topology with minimum of 5 failure domains.
odf_mon_count: 3

# Host networking allows the Ceph cluster to use network interfaces on Kubernetes hosts for communication as opposed to using the pod overlay network.
# This eliminates latency from the software-defined pod network, but it provides no host-level security isolation.
# Host networking sets .spec.hostNetwork = true in Ceph pods. The addressRanges configuration is used to determine an existing node IP address
# that the Ceph components can bind to.
# Link: https://rook.io/docs/rook/latest/CRDs/Cluster/network-providers/#host-networking
#odf_network:
#  provider: host
#  addressRanges:
#    public:
#      - "192.168.40.0/24"
#    cluster:
#      - "192.168.40.0/24"

# Define custom public and cluster networks
#odf_network:
#  provider: multus
#  selectors:
#    public: default/odfpublic
#    cluster: default/odfcluster

# Set this to number of availability (failure) zones
odf_storage_device_set_replica: 3
# Set this to number of disks across all storage nodes in a single zone
odf_storage_device_set_count: 1
# Storage capacity per OSD (tested minimum is 1Gi)
odf_storage_device_set_storage_capacity: 50Gi

odf_platform_to_storageclass:
  AWS:
    mon_pvc_template: gp3-csi
    storage_device_sets: gp3-csi
    mcg_db: gp3-csi
  VSphere:
    mon_pvc_template: thin-csi
    storage_device_sets: thin-csi
    mcg_db: thin-csi
  BareMetal:
    mon_pvc_template: local-hostpath
    # Example storage classes: lvms-loop1-vg, openebs-lvm, lso-myvolumeset1
    storage_device_sets: lvms-loop1-vg
    mcg_db: local-hostpath
  None:
    mon_pvc_template: local-hostpath
    # Example storage classes: lvms-loop1-vg, openebs-lvm, lso-myvolumeset1
    storage_device_sets: lvms-loop1-vg
    mcg_db: local-hostpath

portable_storageclasses:
  - gp3-csi
  - thin-csi

odf_user_hostpath_for_mon_data_dir: false

# If the uninstall.ocs.openshift.io/cleanup-policy is set to "delete",
# the Rook-Ceph storage operator cleans up the physical drives and the DataDirHostPath.
# If the uninstall.ocs.openshift.io/cleanup-policy is set to "retain",
# it keeps the physical drives and DataDirHostPath.
# For production deployments, set uninstallCleanUpPolicy to "retain".
odf_uninstall_cleanup_policy: delete

# If the uninstall.ocs.openshift.io/mode is set to "graceful",
# the Rook-Ceph and NooBaa operators pause the uninstall process
# until the PVCs and OBCs are removed manually.
# If the uninstall.ocs.openshift.io/mode is set to "forced",
# the Rook-Ceph and NooBaa operators proceed with the uninstall process,
# even if the PVCs and the OBCs still exist.
# For production deployments, set uninstallMode to "graceful".
odf_uninstall_mode: forced

odf_encryption_clusterwide: false
odf_encryption_kms_enable: false
